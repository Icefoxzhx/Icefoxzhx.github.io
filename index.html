<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/javascript" src="js/hidebib.js"></script>
    <title>Hongxin Zhang</title>

    <meta name="author" content="Hongxin Zhang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/avatar.jpg" type="image/jpg">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Hongxin Zhang
                </p>
                <p>
                  I am Hongxin Zhang (张洪鑫), a second-year Ph.D. student in Computer Science at <a href="https://www.umass.edu/">UMass Amherst</a>, advised by Prof. <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>.
                </p>
                <p>
                   I received my bachelor’s degree from <a href="https://acm.sjtu.edu.cn/home">ACM Honors Class</a>, <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a>. I was fortunate to be advised by Prof. <a href="https://cs.stanford.edu/~diyiy/index.html">Diyi Yang</a> at <a href="https://www.stanford.edu/">Stanford</a> during my undergraduate.
                </p>
                <p>
                    I am curious about how humans differentiate from other beings and from each other. Currently, I primarily conduct research in <b>Embodied AI</b> and <b>Agents</b>:
                    <ul>
                        <li>How to build embodied agents that reason and social like humans?</li>
                        <li>How to develop agentic systems that best harness the power of foundation models?</li>
                    </ul>
                    My long-term goal is to better understand the human brain by creating systems that mirror its capabilities — and meanwhile, to make robots a part of human society.
                </p>
                  <p>
                      Outside of research, I enjoy reading books, playing billiards and board games.
                  </p>
                <p style="text-align:center">
                  <a href="mailto:hongxinzhang@umass.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/CV_HongxinZhang.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=WM-qkBgAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/icefox1104">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Icefoxzhx/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profile.jpg"><img style="width:100%;max-width:100%;object-fit: cover; " alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Professional Services</h2>
                <ul>
                  <li>Conference Reviewer: ICML'23-25, NeurIPS'23-25, ICLR'24-25, CVPR'24-25, IROS'25, CoRL'25, RSS'25, ARR, COLM'24, IJCAI'24</li>
                    <li>Journal Reviewer: IJCV, TIP</li>
                </ul>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>News</h2>
                <ul>
                    <li>[2025.1] <a href="https://embodied-agi.cs.umass.edu/combo/">COMBO</a> is accepted to ICLR 2025!</li>
                  <li> [2024.5] Interning at Adobe for the summer</li>
                </ul>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
              <td style="padding:20px;width:25%;vertical-align:top">
                  <a href="https://umass-embodied-agi.github.io/combo/"
                  ><video playsinline autoplay loop muted src="images/combo.mp4" alt="sym" width="300" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                  <a href="https://umass-embodied-agi.github.io/combo/">
                      <span class="papertitle">COMBO: Compositional World Models for Embodied Multi-Agent Cooperation</span></a>
                  <br>
                  <strong>Hongxin Zhang*</strong>, Zeyuan Wang*, Qiushi Lyu*, Zheyuan Zhang, Sunli Chen, Tianmin Shu, Yilun Du, Behzad Dariush, Kwonjoon Lee, Chuang Gan
                  <br>
                  <em>ICLR 2025</em>
                  <br>
                  <a href="https://umass-embodied-agi.github.io/combo/">Project Page</a> |
                  <a href="https://arxiv.org/abs/2404.10775">Paper</a> |
                  <a href="https://github.com/UMass-Embodied-AGI/COMBO">Code</a>
              </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <a href="https://embodied-agi.cs.umass.edu/3dmem/"
                  ><video playsinline autoplay loop muted src="images/3dmem_video_intro.mp4" alt="sym" width="300" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://embodied-agi.cs.umass.edu/3dmem/">
                      <span class="papertitle">3D-Mem: 3D Scene Memory for Embodied Exploration and Reasoning</span>
              </a>
              <br>
              Yuncong Yang*, Han Yang*, Jiachen Zhou, Peihao Chen, <strong>Hongxin Zhang</strong>, Yilun Du, Chuang Gan
              <br>
                <em>CVPR 2025</em>
              <br>
              <a href="https://embodied-agi.cs.umass.edu/3dmem/">Project Page</a>
              /
              <a href="https://www.arxiv.org/abs/2411.17735">Paper</a>
              /
              <a href="https://github.com/UMass-Embodied-AGI/3D-Mem">Code</a>
              <p></p>
            </td>
          </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:top">
                  <a href="https://umass-embodied-agi.github.io/CHAIC/">
                      <img src="images/CHAIC.png" alt="img" width="300" height="auto"></a>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                  <a href="https://umass-embodied-agi.github.io/CHAIC/">
                      <span class="papertitle">Constrained Human-AI Cooperation: An Inclusive Embodied Social Intelligence Challenge</span></a>
                  <br>
                  Weihua Du*, Qiushi Lyu*, Jiaming Shan, Zhenting Qi, <strong>Hongxin Zhang</strong>, Sunli Chen, Andi Peng, Tianmin Shu, Kwonjoon Lee, Behzad Dariush, Chuang Gan
                  <br>
                  <em>NeurIPS 2024</em>
                  <br>
                  <a href="https://umass-embodied-agi.github.io/CHAIC/">Project Page</a> |
                  <a href="https://arxiv.org/abs/2411.01796">Paper</a> |
                  <a href="https://github.com/UMass-Embodied-AGI/CHAIC">Code</a>
              </td>
          </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:top">
                <a href="https://umass-embodied-agi.github.io/CoELA/">
<!--                    <img src="images/coela.png" alt="img" width="300" height="auto">-->
                    <video playsinline autoplay loop muted src="images/coela.mp4" alt="sym" width="300" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
                </a>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                <a href="https://umass-embodied-agi.github.io/CoELA/">
                <span class="papertitle">Building Cooperative Embodied Agents Modularly with Large Language Models</span></a>
                <br>
                <strong>Hongxin Zhang*</strong>, Weihua Du*, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B. Tenenbaum, Tianmin Shu, Chuang Gan
                <br>
                <em>ICLR 2024</em>
                <br>
                <a href="https://umass-embodied-agi.github.io/CoELA/">Project Page</a> |
                <a href="https://arxiv.org/abs/2307.02485">Paper</a> |
                <a href="https://github.com/UMass-Embodied-AGI/CoELA/">Code</a> |
              </td>
          </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:top">
                  <a href="https://embodied-agi.cs.umass.edu/hazard/"
                  ><video playsinline autoplay loop muted src="images/hazard.mp4" alt="sym" width="300" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                  <a href="https://embodied-agi.cs.umass.edu/hazard/">
                      <span class="papertitle">HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments</span></a>
                  <br>
                  Qinhong Zhou*, Sunli Chen*, Yisong Wang, Haozhe Xu, Weihua Du, <strong>Hongxin Zhang</strong>, Yilun Du, Joshua B. Tenenbaum, Chuang Gan
                  <br>
                  <em>ICLR 2024</em>
                  <br>
                  <a href="https://embodied-agi.cs.umass.edu/hazard/">Project Page</a> |
                  <a href="https://openreview.net/pdf?id=n6mLhaBahJ">Paper</a> |
                  <a href="https://github.com/UMass-Embodied-AGI/HAZARD">Code</a>
              </td>
          </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2310.05910">
                    <img src="images/salmon_logo.jpeg" alt="img" width="250" height="auto">
                  </a>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2310.05910">
                      <span class="papertitle">SALMON: Self-Alignment with Principle-Following Reward Models</span>
                  </a>
                  <br>
                  Zhiqing Sun, Yikang Shen, <strong>Hongxin Zhang</strong>, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan
                  <br>
                  <em>ICLR 2024</em>
                  <br>
                  <a href="https://arxiv.org/abs/2310.05910">Paper</a> |
                  <a href="https://github.com/IBM/SALMON">Code</a>
              </td>
          </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2305.03047">
                    <img src="images/dromedary_logo_with_text.svg" alt="img" width="250" height="auto">
                  </a>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2305.03047">
                      <span class="papertitle">Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision</span>
                  </a>
                  <br>
                  Zhiqing Sun, Yikang Shen, Qinhong Zhou, <strong>Hongxin Zhang</strong>, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan
                  <br>
                  <em>NeurIPS 2023</em>
                  <br>
                  <a href="https://arxiv.org/abs/2305.03047">Paper</a> |
                  <a href="https://github.com/IBM/Dromedary">Code</a>
              </td>
          </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2212.08061">
                    <img src="images/cot_bias.png" alt="img" width="300" height="auto">
                  </a>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2212.08061">
                      <span class="papertitle">On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning</span>
                  </a>
                  <br>
                  Omar Shaikh, <strong>Hongxin Zhang</strong>, William Held, Michael Bernstein, Diyi Yang
                  <br>
                  <em>ACL 2023</em>
                  <br>
                  <a href="https://arxiv.org/abs/2212.08061">Paper</a> |
                  <a href="https://github.com/SALT-NLP/chain-of-thought-bias">Code</a>
              </td>
          </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:top">
                  <a href="https://aclanthology.org/2023.findings-acl.411.pdf">
                    <img src="images/deduction.png" alt="img" width="300" height="auto">
                  </a>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                  <a href="https://aclanthology.org/2023.findings-acl.411.pdf">
                      <span class="papertitle">Werewolf Among Us: A Multimodal Dataset for Modeling Persuasion Behaviors in Social Deduction Games</span>
                  </a>
                  <br>
                  Bolin Lai*, <strong>Hongxin Zhang*</strong>, Miao Liu*, Aryan Pariani*, Fiona Ryan, Wenqi Jia, Shirley Anugrah Hayati, James M. Rehg, Diyi Yang
                  <br>
                  <em>ACL 2023 Findings</em>
                  <br>
<!--                  <a href="https://persuasion-deductiongame.socialai-data.org/">Project</a> |-->
                  <a href="https://aclanthology.org/2023.findings-acl.411.pdf">Paper</a> |
                  <a href="https://github.com/SALT-NLP/PersuationGames">Code</a>
              </td>
          </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2302.09185">
                    <img src="images/bounding_llm.png" alt="img" width="300" height="auto">
                  </a>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2302.09185">
                      <span class="papertitle">Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</span>
                  </a>
                  <br>
                  Albert Lu*, <strong>Hongxin Zhang*</strong>, Yanzhe Zhang, Xuezhi Wang, Diyi Yang
                  <br>
                  <em>EACL 2023 Findings</em>
                  <br>
                  <a href="https://arxiv.org/abs/2302.09185">Paper</a> |
                  <a href="https://github.com/SALT-NLP/Bound-Cap-LLM">Code</a>
              </td>
          </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2210.10693">
                    <img src="images/RobustDemo.png" alt="img" width="300" height="auto">
                  </a>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2210.10693">
                      <span class="papertitle">Robustness of Demonstration-based Learning Under Limited Data Scenario</span>
                  </a>
                  <br>
                  <strong>Hongxin Zhang</strong>, Yanzhe Zhang, Ruiyi Zhang, Diyi Yang
                  <br>
                  <em>EMNLP 2022</em>
                  <br>
                  <a href="https://arxiv.org/abs/2210.10693">Paper</a> |
                  <a href="https://github.com/SALT-NLP/RobustDemo">Code</a>
              </td>
          </tr>



          </tbody></table>
            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Website's code is from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
  </body>
</html>
